{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Nearest Neighbor Regression and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on the notebook file [01 in AurÃ©lien Geron's github page](https://github.com/ageron/handson-ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import linear_model, neighbors\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "sns.set()\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"..\"\n",
    "datapath = PROJECT_ROOT_DIR + \"/data/lifesat/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CSV from http://stats.oecd.org/index.aspx?DataSetCode=BLI\n",
    "oecd_bli = pd.read_csv(datapath+\"oecd_bli_2015.csv\", thousands=',')\n",
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "\n",
    "oecd_bli.columns\n",
    "\n",
    "oecd_bli[\"Life satisfaction\"].head()\n",
    "\n",
    "# Load and prepare GDP per capita data\n",
    "\n",
    "# Download data from http://goo.gl/j1MSKe (=> imf.org)\n",
    "gdp_per_capita = pd.read_csv(datapath+\"gdp_per_capita.csv\", thousands=',', delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "\n",
    "full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)\n",
    "full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "\n",
    "_ = full_country_stats.plot(\"GDP per capita\",'Life satisfaction',kind='scatter',c=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the full dataset, and there are other columns.  I will subselect a few of them by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['Self-reported health','Water quality','Quality of support network','GDP per capita']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(full_country_stats[xvars])\n",
    "y = np.array(full_country_stats['Life satisfaction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will define the following functions to expedite the LOO risk and the Empirical risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loo_risk(X,y,regmod):\n",
    "    \"\"\"\n",
    "    Construct the leave-one-out square error risk for a regression model\n",
    "    \n",
    "    Input: design matrix, X, response vector, y, a regression model, regmod\n",
    "    Output: scalar LOO risk\n",
    "    \"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(X)\n",
    "    loo_losses = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        regmod.fit(X_train,y_train)\n",
    "        y_hat = regmod.predict(X_test)\n",
    "        loss = np.sum((y_hat - y_test)**2)\n",
    "        loo_losses.append(loss)\n",
    "    return np.mean(loo_losses)\n",
    "\n",
    "def emp_risk(X,y,regmod):\n",
    "    \"\"\"\n",
    "    Return the empirical risk for square error loss\n",
    "    \n",
    "    Input: design matrix, X, response vector, y, a regression model, regmod\n",
    "    Output: scalar empirical risk\n",
    "    \"\"\"\n",
    "    regmod.fit(X,y)\n",
    "    y_hat = regmod.predict(X)\n",
    "    return np.mean((y_hat - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin1 = linear_model.LinearRegression(fit_intercept=False)\n",
    "print('LOO Risk: '+ str(loo_risk(X,y,lin1)))\n",
    "print('Emp Risk: ' + str(emp_risk(X,y,lin1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the empirical risk is much less than the leave-one-out risk!  This can happen in more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method described here: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "\n",
    "I have already imported the necessary module, so you just need to use the regression object (like we used LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn = neighbors.KNeighborsRegressor(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** For each k from 1 to 10 compute the nearest neighbors empirical risk and LOO risk. Plot these as a function of k and reflect on the bias-variance tradeoff here. (Hint: use the previously defined functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Do the same but for the reduced predictor variables below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(full_country_stats[['Self-reported health']])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
